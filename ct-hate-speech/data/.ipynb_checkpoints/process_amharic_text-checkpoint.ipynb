{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from lemmatizer import lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"X_train.json\"]\n",
    "files_processed = [\"X_train_processed.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HiRob üòÇ', 'swiggy', 'üò° üò°']\n"
     ]
    }
   ],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "# search your emoji\n",
    "def is_emoji(s):\n",
    "    return s in UNICODE_EMOJI['en']\n",
    "\n",
    "# add space near your emoji\n",
    "def add_space(text):\n",
    "  result = ''\n",
    "  for char in text:\n",
    "    if is_emoji(char):\n",
    "      result += ' '\n",
    "    result += char\n",
    "  return result.strip()\n",
    "\n",
    "sentences=[\"HiRobüòÇ\",\"swiggy\",\"üò°üò°\"]\n",
    "results=[add_space(text) for text in sentences]\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_amharic(token):\n",
    "    print(\"Token: \" + token)\n",
    "    char_list = list(token)\n",
    "    if (len(char_list) <= 1):\n",
    "        return False\n",
    "    \n",
    "    maxchar = max(list(token))\n",
    "    if u'\\u1200' <= maxchar <= u'\\u137f':\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-5-39d7b73494a1>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-39d7b73494a1>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    r = re.compile(r'([.,#!$%^&*;:{}=_`~()?-]))\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# Remove Punctuations, Remove Hashtags, Remove RT, Remove Mentions, Remove links, Remove numerals, Normalize Whitespace\n",
    "processed_array = []\n",
    "for file_path in files:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        for tweet in data:\n",
    "            r = re.compile(r'([.,#!$%^&*;:{}=_`~()?-])')\n",
    "            #Remove links\n",
    "            tweet = re.sub(\"\\s+\", \" \", tweet) #Normalize Whitespace\n",
    "            tweet = re.sub(\"@\\w*[.:!?\\-\\\"]{0,1}\\s\", \"\", tweet) #Remove Mention\n",
    "            tweet = re.sub(\"https:.*\", \"\", tweet) #Remove links at the end\n",
    "            tweet = re.sub(\"[0-9]*\", \"\", tweet) #Remove numerals\n",
    "            tweet = r.sub(r'', tweet) # Remove Punctuations\n",
    "            tweet = re.sub(\"RT\", \"\", tweet)\n",
    "            tweet = re.sub(\"·ç¢\", \"\", tweet) #Remove Ethiopian period\n",
    "            \n",
    "            tweet_split = tweet.split(\" \")\n",
    "            for idx in range(len(tweet_split)):\n",
    "                tweet_split[idx] = re.sub(\"https:.*\", \"\", tweet_split[idx]) # Remove Link\n",
    "                tweet_split[idx] = re.sub(\"#.*\", \"\", tweet_split[idx]) #Remove Hashtag\n",
    "                inp = tweet_split[idx]\n",
    "                print(\"Input: \" + inp)\n",
    "                if len(tweet_split[idx]) < 2 or detect_amharic(tweet_split[idx]):\n",
    "                    tweet_split[idx] = lemmatize(tweet_split[idx])\n",
    "                \n",
    "                out = tweet_split[idx]\n",
    "                if inp != out:\n",
    "                    print(\"Output: \" + out)\n",
    "                \n",
    "                \n",
    "            tweet = \" \".join(tweet_split)\n",
    "                \n",
    "            tweet += \"\\\"\"\n",
    "            tweet = add_space(tweet)\n",
    "            \n",
    "            # if (tweet == \"\" or len(tweet) < 3):\n",
    "                # tweet = \"\\\"\\n\\\"\"\n",
    "            if len(tweet) == 0 or tweet[-1] != '\\\"':\n",
    "                tweet += '\\\"'\n",
    "            processed_array.append(tweet)\n",
    "    \n",
    "    file_proc = open(files_processed[0], mode=\"w\")\n",
    "    file_proc.write(\"[\\n\")\n",
    "    for proc in processed_array:\n",
    "        file_proc.write(\"\\\"\" + proc + \",\\n\")\n",
    "    file_proc.write(\"]\")\n",
    "    file_proc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
